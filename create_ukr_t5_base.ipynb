{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64746735-6be1-4386-9a85-506ee536b89a",
   "metadata": {},
   "source": [
    "## mT5 model ukrainization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca90129-c684-4a89-8b1d-8814fcc2d5c7",
   "metadata": {},
   "source": [
    "The aim is to compress the mT5-base model to retain Ukrainian embeddings and tokens used for it. We'll still save 10K most popular tokens for English language and 1K most popular tokens overall.\n",
    "\n",
    "An idea and most of the code were taken from [this](https://medium.com/towards-data-science/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90) medium article.\n",
    "\n",
    "Results: \n",
    "- 582M params -> 211M params\n",
    "- 250K tokens -> 8900 tokens\n",
    "- 2.2GB size model -> 0.8GB size model\n",
    "\n",
    "Still, we won't lose much performance if use only the Ukrainian language for our task. This model will be useful for possible training on generated synthetic data and fine-tuned for the GEC task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b21da-b363-499b-82e6-70616d822a0a",
   "metadata": {},
   "source": [
    "### Things we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75dde68-8b19-4204-96c5-f11ce75eb205",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.19.4-py3-none-any.whl (4.2 MB)\n",
      "     ---------------------------------------- 4.2/4.2 MB 4.4 MB/s eta 0:00:00\n",
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.96-cp37-cp37m-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 5.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: importlib-metadata in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers) (4.11.4)\n",
      "Collecting numpy>=1.17\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-win_amd64.whl (14.0 MB)\n",
      "     ---------------------------------------- 14.0/14.0 MB 5.5 MB/s eta 0:00:00\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Downloading tokenizers-0.12.1-cp37-cp37m-win_amd64.whl (3.3 MB)\n",
      "     ---------------------------------------- 3.3/3.3 MB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: requests in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers) (2.28.0)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.64.0-py2.py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.4/78.4 kB 4.3 MB/s eta 0:00:00\n",
      "Collecting filelock\n",
      "  Downloading filelock-3.7.1-py3-none-any.whl (10 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Downloading huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "     ---------------------------------------- 86.2/86.2 kB 4.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from transformers) (21.3)\n",
      "Collecting pyyaml>=5.1\n",
      "  Downloading PyYAML-6.0-cp37-cp37m-win_amd64.whl (153 kB)\n",
      "     -------------------------------------- 153.2/153.2 kB 4.6 MB/s eta 0:00:00\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2022.6.2-cp37-cp37m-win_amd64.whl (262 kB)\n",
      "     -------------------------------------- 262.1/262.1 kB 5.4 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\sanyk\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.7_qbz5n2kfra8p0\\localcache\\local-packages\\python37\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Installing collected packages: tokenizers, sentencepiece, tqdm, regex, pyyaml, numpy, filelock, huggingface-hub, transformers\n",
      "Successfully installed filelock-3.7.1 huggingface-hub-0.7.0 numpy-1.21.6 pyyaml-6.0 regex-2022.6.2 sentencepiece-0.1.96 tokenizers-0.12.1 tqdm-4.64.0 transformers-4.19.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script tqdm.exe is installed in 'C:\\Users\\sanyk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script f2py.exe is installed in 'C:\\Users\\sanyk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script huggingface-cli.exe is installed in 'C:\\Users\\sanyk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The script transformers-cli.exe is installed in 'C:\\Users\\sanyk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907303c8-61a3-4014-aa31-8edd42f840f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanyk\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.7_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python37\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2780\\3254408785.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMT5ForConditionalGeneration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMT5Tokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7bed92-eb27-4510-ad5e-e15cf732cb79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-base\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14fde64-d67d-456b-8768-a0df90a6fe08",
   "metadata": {},
   "source": [
    "Original tokenizer contains 250K tokens and the model has 582M params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c508fdf4-7de8-4dd4-ab56-39c27ca15122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582401280\n",
      "250100\n"
     ]
    }
   ],
   "source": [
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "original_size = msize(model)\n",
    "\n",
    "print(original_size)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4ef37-4144-4445-96e3-7afb72d181c8",
   "metadata": {},
   "source": [
    "#### Ukranian corpus for building our new vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2201b1c-178d-4820-8503-2c34306afab9",
   "metadata": {},
   "source": [
    "Further we'll use the [Ukrainian 2019 corpus](https://wortschatz.uni-leipzig.de/en/download/Ukrainian) (scrapped randomly from web pages) of 1M sentences, taken from University of Leipzig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad90ca21-3f02-4386-bef8-0d7ba741a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "file = tarfile.open('data/ukr-ua_web_2019_1M.tar.gz')\n",
    "\n",
    "fname = 'ukr-ua_web_2019_1M/ukr-ua_web_2019_1M-sentences.txt'\n",
    "\n",
    "file.extract(fname, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0379c20-a07b-48b0-bd86-6b7abd9c48a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>764133</th>\n",
       "      <td>764135</td>\n",
       "      <td>Сьогодні вранці в с. Пядики Коломийського райо...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66617</th>\n",
       "      <td>66619</td>\n",
       "      <td>Бо це по суті є найлогічніше рішення.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95362</th>\n",
       "      <td>95364</td>\n",
       "      <td>Ви і раніше контролювали, недовіряли їй?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578349</th>\n",
       "      <td>578351</td>\n",
       "      <td>Перевіримо це, розглянувши головні мислимі мож...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6057</th>\n",
       "      <td>6059</td>\n",
       "      <td>Smart Solutions у стислі терміни сформує або н...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           idx                                               text\n",
       "764133  764135  Сьогодні вранці в с. Пядики Коломийського райо...\n",
       "66617    66619              Бо це по суті є найлогічніше рішення.\n",
       "95362    95364           Ви і раніше контролювали, недовіряли їй?\n",
       "578349  578351  Перевіримо це, розглянувши головні мислимі мож...\n",
       "6057      6059  Smart Solutions у стислі терміни сформує або н..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df_ua = pd.read_csv('data/' + fname, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "df_ua.columns = ['idx', 'text']\n",
    "df_ua.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cca1a1-d214-47a5-b2e1-863322838398",
   "metadata": {},
   "source": [
    "#### English corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a67a47-3507-4961-8d9c-61b2c75612dd",
   "metadata": {},
   "source": [
    "We'll also use web [corpus](https://wortschatz.uni-leipzig.de/en/download/English) from the same place as we did for Ukrainian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fd7b1d-138d-456d-a9fe-88e317796e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = tarfile.open('data/eng-com_web-public_2018_1M.tar.gz')\n",
    "\n",
    "fname = 'eng-com_web-public_2018_1M/eng-com_web-public_2018_1M-sentences.txt'\n",
    "\n",
    "file.extract(fname, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a379ff98-e269-4450-a21b-4d1ce33c375a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>771355</th>\n",
       "      <td>771357</td>\n",
       "      <td>The place is a little small but it works, we h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393406</th>\n",
       "      <td>393408</td>\n",
       "      <td>“In the end, our sensibilities are on the same...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283109</th>\n",
       "      <td>283111</td>\n",
       "      <td>Hosted by 451 Research, the Hosting &amp; Cloud Tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371022</th>\n",
       "      <td>371024</td>\n",
       "      <td>In addition, the ceiling in the VIP lounge is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876609</th>\n",
       "      <td>876611</td>\n",
       "      <td>Transderma M Moisturizing Serum, - Truth In Ag...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           idx                                               text\n",
       "771355  771357  The place is a little small but it works, we h...\n",
       "393406  393408  “In the end, our sensibilities are on the same...\n",
       "283109  283111  Hosted by 451 Research, the Hosting & Cloud Tr...\n",
       "371022  371024  In addition, the ceiling in the VIP lounge is ...\n",
       "876609  876611  Transderma M Moisturizing Serum, - Truth In Ag..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en = pd.read_csv('data/' + fname, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "df_en.columns = ['idx', 'text']\n",
    "df_en.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77868883-c559-4d20-a4d6-4ac20d654bb9",
   "metadata": {},
   "source": [
    "### Determine new vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0194981-73db-4cfb-9870-3ffe57a12e2b",
   "metadata": {},
   "source": [
    "We tokenize our corpus, count the frequences of different tokens and remain only tokens that were used frequently enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f359ac-f4c4-435f-b161-6a98e4943e59",
   "metadata": {},
   "source": [
    "Count the tokens that the current model uses for representing the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9855c8-8464-48b9-8d27-32f2283c5448",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90966d2d96c045989be0f10d2b6255fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "cnt_ua = Counter()\n",
    "for text in tqdm(df_ua.text):\n",
    "    cnt_ua.update(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e530a16-64a5-4e90-b240-32d936fbb9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49104aeb0df4dff97b3f73d168c914e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/999999 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cnt_en = Counter()\n",
    "for text in tqdm(df_en.text):\n",
    "    cnt_en.update(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e05d5-bf0d-48ac-8deb-6a0518969198",
   "metadata": {},
   "source": [
    "The number of used tokens for our ua corpus is 23% from all mT5 tokenizer vocab size, for en corpus its 27%.\n",
    "\n",
    "There is also 55% overlap between the ua and en vocabularies. The original article assumes that in Russian (our case Ukrainian) text there are occasionaly Emglish words or latin representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b290fc05-f652-4f47-a9dd-ebb6acb6497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58168 0.23257896841263495\n",
      "67920 0.2715713714514194\n",
      "31702 0.5450075642965204\n"
     ]
    }
   ],
   "source": [
    "print(len(cnt_ua), len(cnt_ua)/tokenizer.vocab_size)\n",
    "print(len(cnt_en), len(cnt_en)/tokenizer.vocab_size)\n",
    "common = len(set(cnt_ua.keys()).intersection(set(cnt_en.keys())))\n",
    "print(common, common / len(cnt_ua))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e1e3c-e20c-4b8a-b2d5-d5988bd9239e",
   "metadata": {},
   "source": [
    "For both languages 10K tokens covers about 95% of the vocabulary, and 20K - about 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1802759-516a-4f3a-9293-0523fe0556f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ua\n",
      "10000 0.9807354043937903\n",
      "20000 0.996521760465981\n",
      "30000 0.9986511122211118\n",
      "en\n",
      "10000 0.9531899579723471\n",
      "20000 0.984080976549739\n",
      "30000 0.9937869235026024\n"
     ]
    }
   ],
   "source": [
    "print('ua')\n",
    "for top in 10_000, 20_000, 30_000:\n",
    "    print(top, sum(v for k, v in cnt_ua.most_common(top)) / sum(cnt_ua.values()))\n",
    "print('en')\n",
    "for top in 10_000, 20_000, 30_000:\n",
    "    print(top, sum(v for k, v in cnt_en.most_common(top)) / sum(cnt_en.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452958c0-4994-49c1-8e77-4a22e14cdcb7",
   "metadata": {},
   "source": [
    "Most common tokens. They are mostly prefixes, punctuation or \"little words\" (і, у, й):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04bd902-2632-483c-944f-6c255e6b9fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', ',', '</s>', '.', 'і', '▁в', 'у', 'и', '▁на', '▁з', 'а', 'ів', '▁у', '▁за', 'ї', '▁та', '-', '▁до', '▁не', '▁що', 'ого', '▁по', '▁від', 'я', '▁як', 'о', 'их', 'е', 'й', '▁«']\n",
      "['▁', '</s>', '.', '▁the', ',', 's', '▁to', '▁and', 'a', '▁of', '▁in', '▁is', '▁I', '’', '▁that', 'ed', '▁for', '-', 'ing', \"'\", '▁you', '▁it', '▁with', '▁on', 'ly', 'y', '▁be', '▁The', '▁as', '▁are']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([k for k, v in cnt_ua.most_common(30)]))\n",
    "print(tokenizer.convert_ids_to_tokens([k for k, v in cnt_en.most_common(30)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13500f12-09ce-4b17-b84b-dedc3a4cd7ed",
   "metadata": {},
   "source": [
    "We will do the next composition of vocabulary:\n",
    "- 1K of top tokens of the original tokenizer\n",
    "- Top 10K of the English vocab\n",
    "- Top 20K of the Ukrainian vocab\n",
    "- 100 special tokens that T5 uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd42a4a-b26b-457d-909e-eaff070e572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '</s>', '<unk>', '<0x00>', '<0x01>', '<0x02>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([0,1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0329aed-75c3-49ac-87d7-b3db7eacdb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20919 Ukrainian tokens are included\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "new_tokens = set(range(1000))\n",
    "for i, (k, v) in enumerate(cnt_en.most_common(10_000)):\n",
    "    if k not in new_tokens:\n",
    "        new_tokens.add(k)\n",
    "for i, (k, v) in enumerate(cnt_ua.most_common(25_000)):\n",
    "    if len(new_tokens) == 29_900:\n",
    "        print(i, 'Ukrainian tokens are included')\n",
    "        break\n",
    "    if k not in new_tokens:\n",
    "        new_tokens.add(k)\n",
    "\n",
    "for t in range(tokenizer.vocab_size - 100, tokenizer.vocab_size):\n",
    "    new_tokens.add(t)\n",
    "\n",
    "print(len(new_tokens))\n",
    "kept_ids = sorted(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e09846-c562-46bf-8582-bf9f1447f4c3",
   "metadata": {},
   "source": [
    "The new vocabulary is only 12% percent of the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9c28e96-9b1b-4d63-8172-acf94da86316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11995201919232307"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kept_ids) / tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6d27c-8d99-4a19-972e-798645735228",
   "metadata": {},
   "source": [
    "### Update the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c35c68ba-e1e0-4fb6-bffd-64317d950474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06162760-f394-4efb-95fc-1b00fc958c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size = len(kept_ids)\n",
    "new_emb = torch.nn.Embedding(new_size, model.shared.embedding_dim)\n",
    "new_head = torch.nn.Linear(in_features=model.lm_head.in_features, out_features=new_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f424f0d-d145-498d-833b-8c283d7b363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_id, old_id in enumerate(kept_ids):\n",
    "    new_emb.weight.data[new_id] = model.shared.weight.data[old_id]\n",
    "    new_head.weight.data[new_id] = model.lm_head.weight.data[old_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99b3636a-686a-4d20-bab7-06d407839bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.shared.weight = new_emb.weight\n",
    "model.lm_head.weight = new_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22dc2f-f1db-409a-8f3f-2f52b87fc5c6",
   "metadata": {},
   "source": [
    "The new model has 244M parameters - 42% of the original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d93b4ff-f1d1-420b-95f5-096cac741641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244309248 0.4194861110195362\n"
     ]
    }
   ],
   "source": [
    "print(msize(model), msize(model) / original_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d63e85-982f-4b7e-a7ef-7f244a43c171",
   "metadata": {},
   "source": [
    "### Update the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d8eb4-aec0-449f-9de0-fc572a085d14",
   "metadata": {},
   "source": [
    "From original notebook:\n",
    "> T5 uses Sentencepiece tokenizer, which is implemented in C and is opaque to Python. Fortunately, we can download its model and deploy it into Python using its Protobuf representation.\n",
    "\n",
    "https://github.com/google/sentencepiece/issues/121\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "126b4769-260b-467c-ab85-abdf4540a78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--2022-05-19 18:56:59--  https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.110.133, 185.199.109.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12872 (13K) [text/plain]\n",
      "Saving to: 'sentencepiece_model.proto.1'\n",
      "\n",
      "     0K .......... ..                                         100% 2.00M=0.006s\n",
      "\n",
      "2022-05-19 18:56:59 (2.00 MB/s) - 'sentencepiece_model.proto.1' saved [12872/12872]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c50f22-e135-4ba8-b77d-2aff6a101df7",
   "metadata": {},
   "source": [
    "Compile the protobuf description of the sentencepiece model in order to be able to modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96f22636-aa49-4a7d-81b2-aaf26bf08b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! protoc --python_out=. sentencepiece_model.proto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57162b4a-7e8d-43f0-b79a-f06743271cf6",
   "metadata": {},
   "source": [
    "Serialize the model used by the current tokenizer and open it as a protobuf class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bdd12fa-76a2-4871-b25b-6a0b9c859e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loaded model has pieces: 250100\n",
      "the new pieces: 30000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364f17afe7b7428bba38786ce32ec242",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/220100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece_model_pb2 as spmp\n",
    "smp = tokenizer.sp_model.serialized_model_proto()\n",
    "m = spmp.ModelProto()\n",
    "m.ParseFromString(smp)\n",
    "\n",
    "print('the loaded model has pieces:', len(m.pieces))\n",
    "new_pieces = [m.pieces[idx] for idx in kept_ids]\n",
    "print('the new pieces:', len(new_pieces))\n",
    "\n",
    "# replace the content of the first 30K pieces\n",
    "for i, p in enumerate(new_pieces):\n",
    "    m.pieces[i].piece = p.piece\n",
    "    m.pieces[i].score = p.score\n",
    "    m.pieces[i].type = p.type\n",
    "\n",
    "# drop the remaining pieces\n",
    "n = len(new_pieces)\n",
    "for i in trange(len(m.pieces) - n):\n",
    "    m.pieces.pop(len(m.pieces) - 1)\n",
    "\n",
    "print(len(m.pieces))\n",
    "with open('new_sp.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1c3fcd6-7376-4c12-90f3-0ecefb2561f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = MT5Tokenizer('new_sp.model', extra_ids=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98d236-df32-433f-96b8-b8071073e489",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "20950d4b-a4af-43b5-a5f9-2939a700b7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5Config {\n",
       "  \"_name_or_path\": \"kravchenko/uk-t5-base\",\n",
       "  \"architectures\": [\n",
       "    \"MT5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"mt5\",\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"T5Tokenizer\",\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.__dict__['vocab_size'] = new_size\n",
    "model.config.__dict__['_name_or_path'] = 'kravchenko/uk-t5-base'\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1aa3f29e-20d7-478f-b8f3-099ca4eebc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.__dict__[\"use_cache\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f193316-befc-41a9-8a92-75c46328daff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5Config {\n",
       "  \"_name_or_path\": \"kravchenko/uk-t5-base\",\n",
       "  \"architectures\": [\n",
       "    \"MT5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"mt5\",\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"T5Tokenizer\",\n",
       "  \"torch_dtype\": \"float32\",\n",
       "  \"transformers_version\": \"4.17.0\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "244133b1-7558-4d14-9618-a0e386647bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained('uk-t5-base_local')\n",
    "model.save_pretrained('uk-t5-base_local')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4137dbd-772d-4771-8732-12e51feb9a2f",
   "metadata": {},
   "source": [
    "### Load & test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f98980ee-c615-4c21-abef-891f882c739d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = MT5ForConditionalGeneration.from_pretrained('uk-t5-base_local')\n",
    "tokenizer1 = MT5Tokenizer.from_pretrained('uk-t5-base_local')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b8b25-ebae-4949-ad99-40be3f9fee35",
   "metadata": {},
   "source": [
    "One task our model can \"somehow\" solve is fill the gaps. However, we'll need to finetune this model in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0f80ec74-8640-4c0f-8632-9463c00685c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <extra_id_0> з <extra_id_1> року значно <extra_id_2> сезону, <extra_id_3> до 2020</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
      "<pad> <extra_id_0> з <extra_id_1> року, в районі <extra_id_2> доби <extra_id_3>, <extra_id_4> року <extra_id_5> пропонується подавати...</s>\n",
      "<pad> <extra_id_0> з <extra_id_1> року, <extra_id_2> р-ну <extra_id_3> та минулого</s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer1('Порівнюючи <extra_id_0> відповідним періодом минулого <extra_id_1> покращилася інвестиційна привабливість промислового комплексу району.', return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    hypotheses = model1.generate(\n",
    "        **inputs, \n",
    "        do_sample=True, top_p=0.95, \n",
    "        num_return_sequences=3, \n",
    "        repetition_penalty=2.5,\n",
    "        max_length=32,\n",
    "    )\n",
    "for h in hypotheses:\n",
    "    print(tokenizer1.decode(h))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
