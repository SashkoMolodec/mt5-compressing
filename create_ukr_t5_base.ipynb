{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64746735-6be1-4386-9a85-506ee536b89a",
   "metadata": {},
   "source": [
    "## mT5 model ukrainization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eca90129-c684-4a89-8b1d-8814fcc2d5c7",
   "metadata": {},
   "source": [
    "The aim is to compress the mT5-base model to retain Ukrainian embeddings and tokens used for it. We'll still save 10K most popular tokens for English language and 1K most popular tokens overall.\n",
    "\n",
    "An idea and most of the code were taken from [this](https://medium.com/towards-data-science/how-to-adapt-a-multilingual-t5-model-for-a-single-language-b9f94f3d9c90) medium article.\n",
    "\n",
    "Results for the mt5-small:\n",
    "- 300M params -> 75M params (75%)\n",
    "- 250K tokens -> 8900 tokens\n",
    "- 1.1GB size model -> 0.3GB size model\n",
    "\n",
    "Results for the mt5-base: \n",
    "- 582M params -> 244M params (58%)\n",
    "- 250K tokens -> 8900 tokens\n",
    "- 2.2GB size model -> 0.95GB size model\n",
    "\n",
    "Results for the mt5-large:\n",
    "- 1.2B params -> 779M params (37%)\n",
    "- 250K tokens -> 8900 tokens\n",
    "- 4.6GB size model -> 2.9GB size model\n",
    "\n",
    "Still, we won't lose much performance if use only the Ukrainian language for our task. This model will be useful for possible training on generated synthetic data and fine-tuned for the GEC task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198b21da-b363-499b-82e6-70616d822a0a",
   "metadata": {},
   "source": [
    "### Things we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d75dde68-8b19-4204-96c5-f11ce75eb205",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.19.2)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.96)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.11.4)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.0.12)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "907303c8-61a3-4014-aa31-8edd42f840f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f7bed92-eb27-4510-ad5e-e15cf732cb79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 4.11M/4.11M [00:00<00:00, 6.98MB/s]\n",
      "Downloading: 100%|██████████| 65.0/65.0 [00:00<00:00, 50.3kB/s]\n",
      "Downloading: 100%|██████████| 376/376 [00:00<00:00, 198kB/s]\n",
      "Downloading: 100%|██████████| 702/702 [00:00<00:00, 425kB/s]\n",
      "Downloading: 100%|██████████| 2.17G/2.17G [00:38<00:00, 60.7MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = MT5Tokenizer.from_pretrained(\"google/mt5-base\")\n",
    "model = MT5ForConditionalGeneration.from_pretrained('google/mt5-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14fde64-d67d-456b-8768-a0df90a6fe08",
   "metadata": {},
   "source": [
    "Original tokenizer contains 250K tokens and the model has 582M params."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c508fdf4-7de8-4dd4-ab56-39c27ca15122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "582401280\n",
      "250100\n"
     ]
    }
   ],
   "source": [
    "def msize(m):\n",
    "    return sum(p.numel() for p in m.parameters())\n",
    "\n",
    "original_size = msize(model)\n",
    "\n",
    "print(original_size)\n",
    "print(tokenizer.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c4ef37-4144-4445-96e3-7afb72d181c8",
   "metadata": {},
   "source": [
    "#### Ukranian corpus for building our new vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2201b1c-178d-4820-8503-2c34306afab9",
   "metadata": {},
   "source": [
    "Further we'll use the [Ukrainian 2019 corpus](https://wortschatz.uni-leipzig.de/en/download/Ukrainian) (scrapped randomly from web pages) of 1M sentences, taken from University of Leipzig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ad90ca21-3f02-4386-bef8-0d7ba741a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "file = tarfile.open('data/ukr-ua_web_2019_1M.tar.gz')\n",
    "\n",
    "fname = 'ukr-ua_web_2019_1M/ukr-ua_web_2019_1M-sentences.txt'\n",
    "\n",
    "file.extract(fname, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0379c20-a07b-48b0-bd86-6b7abd9c48a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>741874</th>\n",
       "      <td>741876</td>\n",
       "      <td>Спершись на стіл він, прискіпливо вдивлявся в ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158606</th>\n",
       "      <td>158608</td>\n",
       "      <td>В Росії третина хоч трохи популярних акторів —...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989662</th>\n",
       "      <td>989664</td>\n",
       "      <td>Якщо знаєте лише англійську, варто подумати пр...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78938</th>\n",
       "      <td>78940</td>\n",
       "      <td>Варто зазначити, що на території селищної ради...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473992</th>\n",
       "      <td>473994</td>\n",
       "      <td>Наймолодший з числа патріотів – вихованців Нет...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           idx                                               text\n",
       "741874  741876  Спершись на стіл він, прискіпливо вдивлявся в ...\n",
       "158606  158608  В Росії третина хоч трохи популярних акторів —...\n",
       "989662  989664  Якщо знаєте лише англійську, варто подумати пр...\n",
       "78938    78940  Варто зазначити, що на території селищної ради...\n",
       "473992  473994  Наймолодший з числа патріотів – вихованців Нет..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "df_ua = pd.read_csv('data/' + fname, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "df_ua.columns = ['idx', 'text']\n",
    "df_ua.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cca1a1-d214-47a5-b2e1-863322838398",
   "metadata": {},
   "source": [
    "#### English corpus "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a67a47-3507-4961-8d9c-61b2c75612dd",
   "metadata": {},
   "source": [
    "We'll also use web [corpus](https://wortschatz.uni-leipzig.de/en/download/English) from the same place as we did for Ukrainian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45fd7b1d-138d-456d-a9fe-88e317796e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = tarfile.open('data/eng-com_web-public_2018_1M.tar.gz')\n",
    "\n",
    "fname = 'eng-com_web-public_2018_1M/eng-com_web-public_2018_1M-sentences.txt'\n",
    "\n",
    "file.extract(fname, 'data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a379ff98-e269-4450-a21b-4d1ce33c375a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idx</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>846160</th>\n",
       "      <td>846162</td>\n",
       "      <td>This is what an All-Pro looks like when he’s g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>478376</th>\n",
       "      <td>478378</td>\n",
       "      <td>Jonnie Tyler isn’t like the rest of his small,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147396</th>\n",
       "      <td>147398</td>\n",
       "      <td>Catch the first two episodes during the 8 and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193073</th>\n",
       "      <td>193075</td>\n",
       "      <td>Each site will be analyzed to determine the op...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>629249</th>\n",
       "      <td>629251</td>\n",
       "      <td>Said Adolfo Borgoñó, logistics manager, PINSA,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           idx                                               text\n",
       "846160  846162  This is what an All-Pro looks like when he’s g...\n",
       "478376  478378  Jonnie Tyler isn’t like the rest of his small,...\n",
       "147396  147398  Catch the first two episodes during the 8 and ...\n",
       "193073  193075  Each site will be analyzed to determine the op...\n",
       "629249  629251  Said Adolfo Borgoñó, logistics manager, PINSA,..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_en = pd.read_csv('data/' + fname, sep='\\t', quoting=csv.QUOTE_NONE)\n",
    "df_en.columns = ['idx', 'text']\n",
    "df_en.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77868883-c559-4d20-a4d6-4ac20d654bb9",
   "metadata": {},
   "source": [
    "### Determine new vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0194981-73db-4cfb-9870-3ffe57a12e2b",
   "metadata": {},
   "source": [
    "We tokenize our corpus, count the frequences of different tokens and remain only tokens that were used frequently enough."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f359ac-f4c4-435f-b161-6a98e4943e59",
   "metadata": {},
   "source": [
    "Count the tokens that the current model uses for representing the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c9855c8-8464-48b9-8d27-32f2283c5448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999999/999999 [04:45<00:00, 3496.71it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from tqdm.auto import tqdm, trange\n",
    "\n",
    "cnt_ua = Counter()\n",
    "for text in tqdm(df_ua.text):\n",
    "    cnt_ua.update(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e530a16-64a5-4e90-b240-32d936fbb9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 999999/999999 [04:07<00:00, 4036.40it/s]\n"
     ]
    }
   ],
   "source": [
    "cnt_en = Counter()\n",
    "for text in tqdm(df_en.text):\n",
    "    cnt_en.update(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9e05d5-bf0d-48ac-8deb-6a0518969198",
   "metadata": {},
   "source": [
    "The number of used tokens for our ua corpus is 23% from all mT5 tokenizer vocab size, for en corpus its 27%.\n",
    "\n",
    "There is also 55% overlap between the ua and en vocabularies. The original article assumes that in Russian (our case Ukrainian) text there are occasionaly Emglish words or latin representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b290fc05-f652-4f47-a9dd-ebb6acb6497f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58168 0.23257896841263495\n",
      "67920 0.2715713714514194\n",
      "31702 0.5450075642965204\n"
     ]
    }
   ],
   "source": [
    "print(len(cnt_ua), len(cnt_ua)/tokenizer.vocab_size)\n",
    "print(len(cnt_en), len(cnt_en)/tokenizer.vocab_size)\n",
    "common = len(set(cnt_ua.keys()).intersection(set(cnt_en.keys())))\n",
    "print(common, common / len(cnt_ua))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79e1e3c-e20c-4b8a-b2d5-d5988bd9239e",
   "metadata": {},
   "source": [
    "For both languages 10K tokens covers about 95% of the vocabulary, and 20K - about 99%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1802759-516a-4f3a-9293-0523fe0556f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ua\n",
      "10000 0.9807354043937903\n",
      "20000 0.996521760465981\n",
      "30000 0.9986511122211118\n",
      "en\n",
      "10000 0.9531899579723471\n",
      "20000 0.984080976549739\n",
      "30000 0.9937869235026024\n"
     ]
    }
   ],
   "source": [
    "print('ua')\n",
    "for top in 10_000, 20_000, 30_000:\n",
    "    print(top, sum(v for k, v in cnt_ua.most_common(top)) / sum(cnt_ua.values()))\n",
    "print('en')\n",
    "for top in 10_000, 20_000, 30_000:\n",
    "    print(top, sum(v for k, v in cnt_en.most_common(top)) / sum(cnt_en.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452958c0-4994-49c1-8e77-4a22e14cdcb7",
   "metadata": {},
   "source": [
    "Most common tokens. They are mostly prefixes, punctuation or \"little words\" (і, у, й):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b04bd902-2632-483c-944f-6c255e6b9fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['▁', ',', '</s>', '.', 'і', '▁в', 'у', 'и', '▁на', '▁з', 'а', 'ів', '▁у', '▁за', 'ї', '▁та', '-', '▁до', '▁не', '▁що', 'ого', '▁по', '▁від', 'я', '▁як', 'о', 'их', 'е', 'й', '▁«']\n",
      "['▁', '</s>', '.', '▁the', ',', 's', '▁to', '▁and', 'a', '▁of', '▁in', '▁is', '▁I', '’', '▁that', 'ed', '▁for', '-', 'ing', \"'\", '▁you', '▁it', '▁with', '▁on', 'ly', 'y', '▁be', '▁The', '▁as', '▁are']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([k for k, v in cnt_ua.most_common(30)]))\n",
    "print(tokenizer.convert_ids_to_tokens([k for k, v in cnt_en.most_common(30)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13500f12-09ce-4b17-b84b-dedc3a4cd7ed",
   "metadata": {},
   "source": [
    "We will do the next composition of vocabulary:\n",
    "- 1K of top tokens of the original tokenizer\n",
    "- Top 10K of the English vocab\n",
    "- Top 20K of the Ukrainian vocab\n",
    "- 100 special tokens that T5 uses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fd42a4a-b26b-457d-909e-eaff070e572a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<pad>', '</s>', '<unk>', '<0x00>', '<0x01>', '<0x02>']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.convert_ids_to_tokens([0,1,2,3,4,5]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b0329aed-75c3-49ac-87d7-b3db7eacdb57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20919 Ukrainian tokens are included\n",
      "30000\n"
     ]
    }
   ],
   "source": [
    "new_tokens = set(range(1000))\n",
    "for i, (k, v) in enumerate(cnt_en.most_common(10_000)):\n",
    "    if k not in new_tokens:\n",
    "        new_tokens.add(k)\n",
    "for i, (k, v) in enumerate(cnt_ua.most_common(25_000)):\n",
    "    if len(new_tokens) == 29_900:\n",
    "        print(i, 'Ukrainian tokens are included')\n",
    "        break\n",
    "    if k not in new_tokens:\n",
    "        new_tokens.add(k)\n",
    "\n",
    "for t in range(tokenizer.vocab_size - 100, tokenizer.vocab_size):\n",
    "    new_tokens.add(t)\n",
    "\n",
    "print(len(new_tokens))\n",
    "kept_ids = sorted(new_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e09846-c562-46bf-8582-bf9f1447f4c3",
   "metadata": {},
   "source": [
    "The new vocabulary is only 12% percent of the original one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9c28e96-9b1b-4d63-8172-acf94da86316",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.11995201919232307"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(kept_ids) / tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a6d27c-8d99-4a19-972e-798645735228",
   "metadata": {},
   "source": [
    "### Update the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c35c68ba-e1e0-4fb6-bffd-64317d950474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "06162760-f394-4efb-95fc-1b00fc958c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_size = len(kept_ids)\n",
    "new_emb = torch.nn.Embedding(new_size, model.shared.embedding_dim)\n",
    "new_head = torch.nn.Linear(in_features=model.lm_head.in_features, out_features=new_size, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7f424f0d-d145-498d-833b-8c283d7b363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for new_id, old_id in enumerate(kept_ids):\n",
    "    new_emb.weight.data[new_id] = model.shared.weight.data[old_id]\n",
    "    new_head.weight.data[new_id] = model.lm_head.weight.data[old_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "99b3636a-686a-4d20-bab7-06d407839bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.shared.weight = new_emb.weight\n",
    "model.lm_head.weight = new_head.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da22dc2f-f1db-409a-8f3f-2f52b87fc5c6",
   "metadata": {},
   "source": [
    "The new model has 244M parameters - 42% of the original size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0d93b4ff-f1d1-420b-95f5-096cac741641",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "244309248 0.4194861110195362\n"
     ]
    }
   ],
   "source": [
    "print(msize(model), msize(model) / original_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d63e85-982f-4b7e-a7ef-7f244a43c171",
   "metadata": {},
   "source": [
    "### Update the tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "322d8eb4-aec0-449f-9de0-fc572a085d14",
   "metadata": {},
   "source": [
    "From original notebook:\n",
    "> T5 uses Sentencepiece tokenizer, which is implemented in C and is opaque to Python. Fortunately, we can download its model and deploy it into Python using its Protobuf representation.\n",
    "\n",
    "https://github.com/google/sentencepiece/issues/121\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "126b4769-260b-467c-ab85-abdf4540a78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-06-12 14:41:40--  https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 13451 (13K) [text/plain]\n",
      "Saving to: ‘sentencepiece_model.proto.2’\n",
      "\n",
      "sentencepiece_model 100%[===================>]  13.14K  --.-KB/s    in 0s      \n",
      "\n",
      "2022-06-12 14:41:40 (87.3 MB/s) - ‘sentencepiece_model.proto.2’ saved [13451/13451]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/google/sentencepiece/master/src/sentencepiece_model.proto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c50f22-e135-4ba8-b77d-2aff6a101df7",
   "metadata": {},
   "source": [
    "Compile the protobuf description of the sentencepiece model in order to be able to modify it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96f22636-aa49-4a7d-81b2-aaf26bf08b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "! protoc --python_out=. sentencepiece_model.proto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57162b4a-7e8d-43f0-b79a-f06743271cf6",
   "metadata": {},
   "source": [
    "Serialize the model used by the current tokenizer and open it as a protobuf class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3bdd12fa-76a2-4871-b25b-6a0b9c859e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the loaded model has pieces: 250100\n",
      "the new pieces: 30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 220100/220100 [01:13<00:00, 2999.57it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sentencepiece_model_pb2 as spmp\n",
    "smp = tokenizer.sp_model.serialized_model_proto()\n",
    "m = spmp.ModelProto()\n",
    "m.ParseFromString(smp)\n",
    "\n",
    "print('the loaded model has pieces:', len(m.pieces))\n",
    "new_pieces = [m.pieces[idx] for idx in kept_ids]\n",
    "print('the new pieces:', len(new_pieces))\n",
    "\n",
    "# replace the content of the first 30K pieces\n",
    "for i, p in enumerate(new_pieces):\n",
    "    m.pieces[i].piece = p.piece\n",
    "    m.pieces[i].score = p.score\n",
    "    m.pieces[i].type = p.type\n",
    "\n",
    "# drop the remaining pieces\n",
    "n = len(new_pieces)\n",
    "for i in trange(len(m.pieces) - n):\n",
    "    m.pieces.pop(len(m.pieces) - 1)\n",
    "\n",
    "print(len(m.pieces))\n",
    "with open('new_sp.model', 'wb') as f:\n",
    "    f.write(m.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b1c3fcd6-7376-4c12-90f3-0ecefb2561f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer = MT5Tokenizer('new_sp.model', extra_ids=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac98d236-df32-433f-96b8-b8071073e489",
   "metadata": {},
   "source": [
    "### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20950d4b-a4af-43b5-a5f9-2939a700b7ed",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5Config {\n",
       "  \"_name_or_path\": \"kravchenko/uk-mt5-base\",\n",
       "  \"architectures\": [\n",
       "    \"MT5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"mt5\",\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"T5Tokenizer\",\n",
       "  \"transformers_version\": \"4.19.2\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config.__dict__['vocab_size'] = new_size\n",
    "model.config.__dict__['_name_or_path'] = 'kravchenko/uk-mt5-base'\n",
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1aa3f29e-20d7-478f-b8f3-099ca4eebc6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.__dict__[\"use_cache\"] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8f193316-befc-41a9-8a92-75c46328daff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MT5Config {\n",
       "  \"_name_or_path\": \"kravchenko/uk-mt5-base\",\n",
       "  \"architectures\": [\n",
       "    \"MT5ForConditionalGeneration\"\n",
       "  ],\n",
       "  \"d_ff\": 2048,\n",
       "  \"d_kv\": 64,\n",
       "  \"d_model\": 768,\n",
       "  \"decoder_start_token_id\": 0,\n",
       "  \"dropout_rate\": 0.1,\n",
       "  \"eos_token_id\": 1,\n",
       "  \"feed_forward_proj\": \"gated-gelu\",\n",
       "  \"initializer_factor\": 1.0,\n",
       "  \"is_encoder_decoder\": true,\n",
       "  \"layer_norm_epsilon\": 1e-06,\n",
       "  \"model_type\": \"mt5\",\n",
       "  \"num_decoder_layers\": 12,\n",
       "  \"num_heads\": 12,\n",
       "  \"num_layers\": 12,\n",
       "  \"output_past\": true,\n",
       "  \"pad_token_id\": 0,\n",
       "  \"relative_attention_max_distance\": 128,\n",
       "  \"relative_attention_num_buckets\": 32,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"tokenizer_class\": \"T5Tokenizer\",\n",
       "  \"transformers_version\": \"4.19.2\",\n",
       "  \"use_cache\": false,\n",
       "  \"vocab_size\": 30000\n",
       "}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "244133b1-7558-4d14-9618-a0e386647bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tokenizer.save_pretrained('uk-t5-base_local')\n",
    "model.save_pretrained('uk-t5-base_local')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4137dbd-772d-4771-8732-12e51feb9a2f",
   "metadata": {},
   "source": [
    "### Load & test new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f98980ee-c615-4c21-abef-891f882c739d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 753/753 [00:00<00:00, 444kB/s]\n",
      "Downloading: 100%|██████████| 2.90G/2.90G [02:53<00:00, 17.9MB/s]   \n",
      "Downloading: 100%|██████████| 785k/785k [00:00<00:00, 1.78MB/s]\n",
      "Downloading: 100%|██████████| 65.0/65.0 [00:00<00:00, 37.0kB/s]\n",
      "Downloading: 100%|██████████| 173/173 [00:00<00:00, 90.3kB/s]\n"
     ]
    }
   ],
   "source": [
    "model1 = MT5ForConditionalGeneration.from_pretrained('kravchenko/uk-mt5-large')\n",
    "tokenizer1 = MT5Tokenizer.from_pretrained('kravchenko/uk-mt5-large')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155b8b25-ebae-4949-ad99-40be3f9fee35",
   "metadata": {},
   "source": [
    "One task our model can \"somehow\" solve is fill the gaps. However, we'll need to finetune this model in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0f80ec74-8640-4c0f-8632-9463c00685c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> <extra_id_0> з <extra_id_1> століття, значно <extra_id_2> року, помітно <extra_id_3> із <extra_id_18> розвитку, істочно <extra_id_7>. Реально <extra_id_8> сторічч\n",
      "<pad> <extra_id_0> з <extra_id_1> року, помітно <extra_id_2> століть значно <extra_id_3> із <extra_id_4> десятиріччя, значним чином <extra_id_5> зі <extra_id_6> за <extra_id_17> стан\n",
      "<pad> <extra_id_0> з <extra_id_1> року, суттєво <extra_id_2> десятиріччя значно <extra_id_3> з <extra_id_4> століття, значною мірою <extra_id_5> зі <extra_id_6>\n"
     ]
    }
   ],
   "source": [
    "inputs = tokenizer1('Порівнюючи <extra_id_0> відповідним періодом минулого <extra_id_1> покращилася інвестиційна привабливість промислового комплексу району.', return_tensors='pt')\n",
    "with torch.no_grad():\n",
    "    hypotheses = model1.generate(\n",
    "        **inputs, \n",
    "        do_sample=True, top_p=0.95, \n",
    "        num_return_sequences=3, \n",
    "        repetition_penalty=2.5,\n",
    "        max_length=32,\n",
    "    )\n",
    "for h in hypotheses:\n",
    "    print(tokenizer1.decode(h))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "common-cu110.m93",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cu110:m93"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
